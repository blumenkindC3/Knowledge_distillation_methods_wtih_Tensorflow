{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9v6gNRNF2QS"
   },
   "source": [
    "# Example code for training network with knowledge distillation\n",
    "This tutorial explains the code to train network with knowledge distillation. And to make easy to train a new model  yourself this tutorial is using Google Colab.\n",
    "\n",
    "Most of Knowledge distillation algorithm's training procedure is categorized in two manners. \n",
    "\n",
    "The first manner is initializing student network by teacher's knowledge such as FitNet, FSP, AB and so on. So their training procedure is composed by training teacher network, initializing student network and finetuning student network.\n",
    "\n",
    "The second manner is multi-task learning which learn main-task and teacher's knowledge at the same time such as Soft-logits, AT, KD-SVD and so on. And their training procedure is composed by training teacher network and training student network with teacher knowledge.\n",
    "\n",
    "But to make both training procedure possible by just one training code, I combine the initializing step and the finetuning step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYxZQrgyKROU"
   },
   "source": [
    "# Cloning the Github codes\n",
    "The first step is cloning Github code repo. After running the bellow code you will find codes in 'File' tap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trlir3CQG1Sv"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/sseung0703/Knowledge_distillation_methods_wtih_Tensorflow.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KNm4Qv6gGA3x"
   },
   "source": [
    "#Training teacher network\n",
    "The next step is training the teacher network. teacher network is trained without any Distillation method. And define the main scope name as 'Teacher' to make easy to assign teacher parameters.\n",
    "below code is example to train new teacher network.\n",
    "\n",
    "In Google colab it takes about 2 hours. So if you have not enough time, you can skip this pass and use the trained parameter I uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORMTub2KJljo",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From train_w_distill.py:206: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/clara/.local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/clara/.local/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:2557: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From train_w_distill.py:201: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-06-16 11:47:43.746386: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-06-16 11:47:43.780316: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1800000000 Hz\n",
      "2019-06-16 11:47:43.781331: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x25c9760 executing computations on platform Host. Devices:\n",
      "2019-06-16 11:47:43.781370: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "INFO:tensorflow:Epoch 000 Step 000001 - train_Accuracy : 1.00%  val_Accuracy : 1.00%\n",
      "INFO:tensorflow:global step 000200: loss = 7.7447 (13.419 sec/step)\n",
      "INFO:tensorflow:Epoch 001 Step 000391 - train_Accuracy : 7.07%  val_Accuracy : 11.20%\n",
      "INFO:tensorflow:global step 000400: loss = 6.6260 (13.585 sec/step)\n",
      "INFO:tensorflow:global step 000600: loss = 5.8747 (13.619 sec/step)\n",
      "INFO:tensorflow:Epoch 002 Step 000782 - train_Accuracy : 16.46%  val_Accuracy : 22.01%\n",
      "INFO:tensorflow:global step 000800: loss = 5.2239 (13.569 sec/step)\n",
      "INFO:tensorflow:global step 001000: loss = 4.6897 (13.587 sec/step)\n",
      "INFO:tensorflow:Epoch 003 Step 001172 - train_Accuracy : 25.52%  val_Accuracy : 28.98%\n",
      "INFO:tensorflow:global step 001200: loss = 4.2446 (13.686 sec/step)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python3 train_w_distill.py --Distillation=None --train_dir=test --main_scope=Teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZnnsjlNyKGep"
   },
   "source": [
    "When training is done, you can find the trained parameter which named 'train_params.mat' in training directory.\n",
    "So copy that file to 'pre_trained' directory. So copy that file to 'pre_trained' directory by bellow code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pFrkaRI7Mgfb"
   },
   "outputs": [],
   "source": [
    "! cp test/train_params.mat Knowledge_distillation_methods_wtih_Tensorflow/pre_trained/ResNet_teacher.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0VUzEWcMyG2"
   },
   "source": [
    "#Training Student Network with Teacher's Knowledge\n",
    "Finally,  we can train a student network with teacher knowledge. To use teacher network's parameter we have to define the name of the saved parameter and Distillation method.\n",
    "For example, I define name as ResNet_teacher which defined above and Distillation method as RKD which is the latest method in my experiments. In my experiment, KD-SVD is the best method, but KD-SVD is the slowest one in implemented methods. And Google colab's memory bottleneck is worse than real hardware. So if you want to try KD-SVD you should try in your own's hardware.  \n",
    "\n",
    "And if you use provided weights, remove teacher FLAG.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YsMHosFhN7YC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From train_w_distill.py:206: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/clara/.local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/clara/.local/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:2557: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From train_w_distill.py:201: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-06-16 19:47:46.399585: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-06-16 19:47:46.440317: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1800000000 Hz\n",
      "2019-06-16 19:47:46.441069: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x25e2710 executing computations on platform Host. Devices:\n",
      "2019-06-16 19:47:46.441109: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/clara/.local/lib/python3.6/site-packages/scipy/io/matlab/mio.py\", line 39, in _open_file\n",
      "    return open(file_like, mode), True\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/clara/deeplearning/Knowledge_distillation_methods_wtih_Tensorflow-master/pre_traines/ResNet32.mat'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"train_w_distill.py\", line 228, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/home/clara/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
      "    _sys.exit(main(argv))\n",
      "  File \"train_w_distill.py\", line 104, in main\n",
      "    teacher = sio.loadmat(home_path + '/pre_traines/%s.mat'%FLAGS.teacher)\n",
      "  File \"/home/clara/.local/lib/python3.6/site-packages/scipy/io/matlab/mio.py\", line 216, in loadmat\n",
      "    with _open_file_context(file_name, appendmat) as f:\n",
      "  File \"/usr/lib/python3.6/contextlib.py\", line 81, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"/home/clara/.local/lib/python3.6/site-packages/scipy/io/matlab/mio.py\", line 19, in _open_file_context\n",
      "    f, opened = _open_file(file_like, appendmat, mode)\n",
      "  File \"/home/clara/.local/lib/python3.6/site-packages/scipy/io/matlab/mio.py\", line 45, in _open_file\n",
      "    return open(file_like, mode), True\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/clara/deeplearning/Knowledge_distillation_methods_wtih_Tensorflow-master/pre_traines/ResNet32.mat'\n"
     ]
    }
   ],
   "source": [
    "!python3 train_w_distill.py --Distillation=RKD --train_dir=kdsvd --main_scope=Student_w_KD-SVD --teacher=ResNet32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tutorial.ipynb의 사본",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
